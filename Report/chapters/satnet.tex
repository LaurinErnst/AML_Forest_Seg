After implementing the U-Net, we wanted to try a slightly different approach of using convolutional networks, so we researched other methods used in satellite image analysis and found a paper (see \textbf{\color{red} zitieren}) segmentating satellite images using a so called "SatNet". They used the SatNet to identify roads and buildings on satellite images, which is not too far from our task, so we used the structure they provided in the paper (\ref{satnet_structure}). \\

\section{Introduction to SatNet}
\begin{figure}
  \begin{center}
  \label{satnet_structure}
  \includegraphics[width=.7\linewidth]{images/satnet_structure}
  \caption{Structure of the SatNet, taken from \textbf{\color{red} Satnetzitieren}}
  \end{center}
\end{figure}

As explained in \textbf{\color{red}zitieren}, the SatNet is heavily inspired by the ResNet, because in the SatNet there are few connections which do not add the input to the convoluted output. First of all, this reduces the risk of vanishing gradients, because no matter the learned function, the identity still provides a large gradient for the top layers. Second, the skip connections allow the SatNet to be much deeper and have much more layers than other ConvNets, because it is very easy for the layers to learn the identity function and thus, these layers can extract information only if necessary.

From an overall structure, the SatNet still encodes and then decodes the images, similarly to the U-Net. The big difference here is that the SatNet downsamples the images by striding instead of pooling. In our structure, we have two convolution layers with a stride parameter of two, the others all keep the dimension constant. After the encoding, the SatNet quickly upsamples the codes with two transposed convolutions also using a stride parameter of two. The advantage of downsampling with stride is its efficiency in computation, because such a layer downsamples and convolutes the data at the same time and with less parameters.

The SatNet is designed such that after every downsampling there are a lot of convolution layers without reducing the dimension. This allows the network to "adjust" to the loss of information and gives it time to finely extract all information from the new downsampled data before downsampling it again. Overall, SatNet does not reduce the data as far as the U-Net does, because the lowest dimension SatNet reaches is a 63x63 grid as opposed to the 28x28 grid of the U-Net. However, the SatNet keeps the number of channels used low, increasing the number only when downsampling, contrary to what the U-Net does. So the best way to describe the approach of the SatNet is that it takes few features (channels) and refines those in an optimal way to keep all important information, whereas the U-Net uses a lot of channels to keep as much information as possible.

Looking at the details, ReLU is applied to the output of every convolution layer to keep the inputs positive. The exception is the last layer where a sigmoid function is applied to give a result between $0$ and $1$, providing a probability of a pixel being forest. Obviously, the last layer only needs one channel for segmentation.

To improve training, a batch normalisation is performed after every convolution to keep the values centered and numerically stable. Furthermore, there is one dropout layer at the end of the encoder, which is enough to prevent overfitting, because this is where the information loss by dropping a neuron is the highest due to the information being most dense in the encoded data.

To summarise, we expect the SatNet to be faster in training, because it has significantly less channels and thus trainable parameters than the U-Net: The U-Net we trained has approximately 1.8 million parameters whereas the SatNet only has around 700k. Thus, the performance ceiling of the SatNet is expected to be lower than the U-Net's but maybe it can achieve better results in less time. \\

\section{Training}
The training algorith for the SatNet does not differ from the one we used for the U-Net; we had to different optimisers, namely the ADAM algorithm and the classical SGD and tried the BCE loss as well as the MSE loss. The same procedure was applied, going through multiple runs to avoid getting stuck in a local minimum. However, because of time troubles, we reduced the amount of runs. Also, the runs are expected to be faster than the U-Net runs, because we have less parameters und thus the gradients become easier to compute. As mentioned before, this of course means that the SatNet could perform worse than the U-Net, but a faster training is ideal when facing a lack of time.

One training parameter which we do not have in the U-Net training is the dropout probability of the dropout layer in the SatNet, however, we did not experiment with it and left it at the standard $p_{drop}=0.5$. \\

\section{Results}
bad on img with little to no forest\\

\section{Interesting Data}

After training the SatNet, we let it run over the whole dataset and picked out some of the images where the model performed best, worst and most average on (w.r.t. the Jaccard index), to get a deeper understanding of the SatNet's strengths and weaknesses.

\subsubsection{Strengths}

Starting with the strengths, the SatNet performes best on images which are just plain forest, see \ref{satnet_best}. This is to be expected, because the picture is basically only trees, which makes it easy for the model to assign all pixels to forest.

\subsubsection{Weaknesses}

\begin{figure}[!h]
  \begin{center}
  \label{satnet_best}
  \includegraphics[width=.4\linewidth]{images/satellite_images/sat_best_real}
  \includegraphics[width=.4\linewidth]{images/satellite_images/sat_best_calc}
  \caption{Image where SatNet performed best on; left is the image with the real mask and the calculated mask on the right. It fits perfectly}
  \end{center}
\end{figure}

For the SatNet performances, there are two examples which represent most of what went wrong when the SatNet misclassified almost all pixels in an image. In the first example, as mentioned in the Dataset chapter, in some of the images in the dataset it is not very clear if there is forest or just bushes and our model performed quite bad on those. In \ref{satnet_worst1}, one could make an argument that the green parts are bushes and not really forest, but the segmentation of our model seems more appropriate than claiming that there is no forest at all. In addition to that, the SatNet actually covered the forest really accurately, which makes it unfortunate that this is an image with one of the worst scores.

\begin{figure}
  \begin{center}
  \label{satnet_worst1}
  \includegraphics[width=.4\linewidth]{images/satellite_images/sat_worst_1}
  \includegraphics[width=.4\linewidth]{images/satellite_images/sat_worst_2}
  \caption{Image where SatNet performed worst on; left is the image with the real mask (no pixels are classified as forest) and the calculated mask on the right}
  \end{center}
\end{figure}

Then again, bad performances also occur on images where the given masks are completely reasonable. In these cases, it often is fields the SatNet struggles with, maybe because they form a coherent shape, which forests also do, and often have a brownish color, which is not too far from a forest either. In \ref{satnet_worst2}, we can see that a lot of the fields, which the true mask correctly does not identify as forest, are claimed to be forest by the SatNet. Again, the argument could be made that our model did a better job than the true mask in the top right corner but that is besides the point. \\

\begin{figure}
\begin{center}
  \label{satnet_worst2}
  \includegraphics[width=.3\linewidth]{images/satellite_images/sat_worst2_1}
  \includegraphics[width=.3\linewidth]{images/satellite_images/sat_worst2_2}
  \includegraphics[width=.3\linewidth]{images/satellite_images/sat_worst2_3}
  \caption{Image where SatNet performed worst on; left is the image with the real mask (no pixels are classified as forest) and the calculated mask in the middle with its outline on the right}
  \end{center}
\end{figure}

\subsubsection{Average Performance}
