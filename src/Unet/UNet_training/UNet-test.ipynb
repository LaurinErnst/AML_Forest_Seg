{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from data_handling import batchloader as loader\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_directory = 'data/images/'\n",
    "mask_directory = 'data/masks/'\n",
    "\n",
    "def batchloader(batchsize = None, batch = None):\n",
    "\n",
    "    # if batch is given iterate throught batch indices\n",
    "    if batchsize == None and batch != None:\n",
    "        j = 0\n",
    "        for i in batch:\n",
    "            # open image\n",
    "            f_im = im_directory + str(i) + \".jpg\"\n",
    "            f_mask = mask_directory + str(i) + \".jpg\"\n",
    "            im = Image.open(f_im)\n",
    "            mask = Image.open(f_mask)\n",
    "            mask= mask.convert('L')\n",
    "            \n",
    "            # extract data into numpy array  \n",
    "            im_data = np.array(im.getdata()).T\n",
    "            mask_data = np.array(mask.getdata()).T\n",
    "\n",
    "            # get data into right shape and concat image data to final data array\n",
    "            if j == 0:\n",
    "\n",
    "                data_im = np.array([im_data.reshape(3, 256, 256)])\n",
    "                data_mask = np.array([mask_data.reshape(1, 256, 256)])\n",
    "                j += 1\n",
    "\n",
    "            else:\n",
    "                im_data = im_data.reshape(1, 3, 256, 256)\n",
    "                data_im = np.concatenate((data_im, im_data))\n",
    "\n",
    "                mask_data = mask_data.reshape(1, 1, 256, 256)\n",
    "                data_mask = np.concatenate((data_mask, mask_data))\n",
    "        \n",
    "        return torch.tensor(data_im).float(), torch.tensor(data_mask).float()\n",
    "\n",
    "    if batchsize != None and batch == None:\n",
    "\n",
    "        # if batch is not given generate random batch of size batchsize\n",
    "        batch = np.random.randint(5108, size = batchsize)\n",
    "        j = 0\n",
    "        for i in batch:\n",
    "            # open image\n",
    "            f_im = os.path.join(im_directory, str(i) + \".jpg\")\n",
    "            f_mask = os.path.join(mask_directory, str(i) + \".jpg\")\n",
    "            im = Image.open(f_im)\n",
    "            mask = Image.open(f_mask)\n",
    "            mask= mask.convert('L')\n",
    "            \n",
    "            # extract data into numpy array  \n",
    "            im_data = np.array(im.getdata()).T\n",
    "            mask_data = np.array(mask.getdata()).T\n",
    "\n",
    "            # get data into right shape and concat image data to final data array\n",
    "            if j == 0:\n",
    "\n",
    "                data_im = np.array([im_data.reshape(3, 256, 256)])\n",
    "                data_mask = np.array([mask_data.reshape(1, 256, 256)])\n",
    "                j += 1\n",
    "\n",
    "            else:\n",
    "                im_data = im_data.reshape(1, 3, 256, 256)\n",
    "                data_im = np.concatenate((data_im, im_data))\n",
    "\n",
    "                mask_data = mask_data.reshape(1, 1, 256, 256)\n",
    "                data_mask = np.concatenate((data_mask, mask_data))\n",
    "        \n",
    "        return torch.tensor(data_im).float(), torch.tensor(data_mask).float()\n",
    "\n",
    "    if batchsize == None and batch == None:\n",
    "        # if batchsize and batch are None raise Exception\n",
    "        raise Exception(\"Weder batchsize noch batch deklariert!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv2(self.relu(self.conv1(x)))\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, chs=(3,64,128,256)):\n",
    "        super().__init__()\n",
    "        self.enc_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n",
    "        self.pool       = nn.MaxPool2d(2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ftrs = []\n",
    "        for block in self.enc_blocks:\n",
    "            x = block(x)\n",
    "            ftrs.append(x)\n",
    "            x = self.pool(x)\n",
    "        return ftrs\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, chs=(256, 128, 64)):\n",
    "        super().__init__()\n",
    "        self.chs         = chs\n",
    "        self.upconvs    = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i+1], 2, 2) for i in range(len(chs)-1)])\n",
    "        self.dec_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)]) \n",
    "        \n",
    "    def forward(self, x, encoder_features):\n",
    "        for i in range(len(self.chs)-1):\n",
    "            x        = self.upconvs[i](x)\n",
    "            enc_ftrs = self.crop(encoder_features[i], x)\n",
    "            x        = torch.cat([x, enc_ftrs], dim=1)\n",
    "            x        = self.dec_blocks[i](x)\n",
    "        return x\n",
    "    \n",
    "    def crop(self, enc_ftrs, x):\n",
    "        _, _, H, W = x.shape\n",
    "        enc_ftrs   = torchvision.transforms.CenterCrop([H, W])(enc_ftrs)\n",
    "        return enc_ftrs\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, enc_chs=(3,64,128,256), dec_chs=(256, 128, 64), num_class=1, retain_dim=False, out_sz=(256,256)):\n",
    "        super().__init__()\n",
    "        self.encoder     = Encoder(enc_chs)\n",
    "        self.decoder     = Decoder(dec_chs)\n",
    "        self.head        = nn.Conv2d(dec_chs[-1], num_class, 1)\n",
    "        self.retain_dim  = retain_dim\n",
    "        self.out_sz = out_sz\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_ftrs = self.encoder(x)\n",
    "        out      = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])\n",
    "        out      = self.head(out)\n",
    "        if self.retain_dim:\n",
    "            out = F.interpolate(out, self.out_sz)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = UNet(retain_dim=True)\n",
    "\n",
    "lossFunc = torch.nn.MSELoss()\n",
    "opt = Adam(Model.parameters())\n",
    "\n",
    "print(\"Setup fertig\")\n",
    "\n",
    "train_steps = 10\n",
    "test_steps = 0\n",
    "\n",
    "# initialize a dictionary to store training historys\n",
    "H = {\"train_loss\": [], \"test_loss\": []}\n",
    "\n",
    "for e in range(10):\n",
    "    # set the model in training mode\n",
    "    Model.train()\n",
    "    # initialize the total training and validation loss\n",
    "    totalTrainLoss = 0\n",
    "    totalTestLoss = 0\n",
    "\n",
    "    # loop over the training set\n",
    "    for i in range(train_steps):\n",
    "\n",
    "        x, y = loader.batchloader(10)\n",
    "\n",
    "        # perform a forward pass and calculate the training loss\n",
    "        pred = Model.forward(x)\n",
    "        loss = lossFunc(pred, y)\n",
    "        # first, zero out any previously accumulated gradients, then\n",
    "        # perform backpropagation, and then update model parameters\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        # add the loss to the total training loss so far\n",
    "        totalTrainLoss += loss\n",
    "\n",
    "    # calculate the average training and validation loss\n",
    "    avgTrainLoss = totalTrainLoss / train_steps\n",
    "    # update our training history\n",
    "    H[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "    # print the model training and validation information\n",
    "    print(\"[INFO] EPOCH: {}/{}\".format(e + 1, 10))\n",
    "    print(\"Train loss: {:.6f}\".format(\n",
    "    avgTrainLoss))\n",
    "\n",
    "# plot the training loss\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(H[\"train_loss\"], label=\"train_loss\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
